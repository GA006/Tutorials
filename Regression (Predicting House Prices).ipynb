{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston House Prices - Regression Task <a class=\"anchor\" id=\"boston-home-anchor\"></a>\n",
    "* [Loading the Data](#boston-load-anchor)\n",
    "* [Manipulating the Data](#boston-manipulate-anchor)\n",
    "* [Building the Model](#boston-build-anchor)\n",
    "* [Evaluating the Model](#boston-evaluate-anchor)\n",
    "\n",
    "In this notebook, I will walk you through the task of predicting housing prices via regression. \n",
    "## Loading the Data <a class=\"anchor\" id=\"boston-load-anchor\"></a>\n",
    "[home](#boston-home-anchor)\n",
    "\n",
    "The dataset we use for this task is the well-known *Boston Housing Prices Dataset* which consists of 404 training samples and 102 testing samples. Each sample consists of 13 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating the Data <a class=\"anchor\" id=\"boston-manipulate-anchor\"></a>\n",
    "[home](#boston-home-anchor)\n",
    "\n",
    "When there are different features which have very different scales, we need to normalize the dataset in order to make the learning process easier. We **normalize** the dataset by taking the mean and the standard deviation of each feature in the training set, then we substitute the mean and divide by the standard deviation. Note that we take the mean and std of the training set only because we don't want to use the testing data during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "std  = train_data.std(axis=0)\n",
    "train_data -= mean\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "For the task at hand we use a neural network model which consists of an input layer, two hidden layers and an output layer. Each hidden layer has 64 nodes. We have 404 training samples, each consisting of 13 features. In other words, we don't have much data so we shouldn't build a neural network with a lot of layers and nodes because it leads to *overfitting*. There isn't an activation function for the output layer because we have a regression task and we don't want an activation function to put a constraint on the range of values which the output layer can take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "We want to split the training set into a train set and a validation set so that we can find the optimal hyperparameters - number of epochs, batch size, etc. However, in this example there are only 404 training samples which means that the choice of a validation set will greatly influence the results, so we will use **K-fold Cross-Validation**. Below is the code of how to implement the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold # 0\n",
      "Processing fold # 1\n",
      "Processing fold # 2\n",
      "Processing fold # 3\n"
     ]
    }
   ],
   "source": [
    "k               = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs      = 100\n",
    "all_scores      = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('Processing fold #', i)\n",
    "    val_data    = train_data[i * num_val_samples: (i+1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i+1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data    = np.concatenate([train_data[:i * num_val_samples], train_data[(i+1) * num_val_samples:]], axis=0)\n",
    "    partial_train_targets = np.concatenate([train_targets[:i * num_val_samples], train_targets[(i+1) * num_val_samples:]], axis=0)\n",
    "    \n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    \n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.37664994597435"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teXnet_predictor_gpu_env]",
   "language": "python",
   "name": "conda-env-teXnet_predictor_gpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
